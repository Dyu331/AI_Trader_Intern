{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bd0a702-0364-4408-af8d-a2ed69e86291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "BASE_DIR=\"/Users/dannyyu/Desktop/AI_Trader/data\"\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'INTC', 'AMD', 'IBM']\n",
    "START_DATE = \"2022-01-01\"\n",
    "END_DATE = \"2025-5-18\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2dcee39-2f60-4c08-9c4f-7fe8c869bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns the most recent available closing price on or before target_date\n",
    "by checking historical data within the past `lookback_days`.\n",
    "\"\"\"\n",
    "def get_closest_price(ticker_obj, target_date, lookback_days=3):\n",
    "    try:\n",
    "        start_date = target_date - pd.Timedelta(days=lookback_days)\n",
    "        end_date = target_date + pd.Timedelta(days=1)\n",
    "\n",
    "        hist = ticker_obj.history(start=start_date, end=end_date)\n",
    "        \n",
    "        if not hist.empty:\n",
    "            last_row = hist.iloc[-1]\n",
    "            timestamp_str = hist.index[-1].strftime(\"%Y-%m-%d\")\n",
    "           # print(\"Data retrieved on date \"+timestamp_str)\n",
    "            return last_row['Close']\n",
    "        else:\n",
    "            print(f\"No price data found for {ticker_obj.ticker} between {start_date.date()} and {end_date.date()}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for {ticker_obj.ticker} near {target_date.date()}: {e}\")\n",
    "        return None\n",
    "\n",
    "def read_CSV_File(ticker,folderName):\n",
    "    dir = BASE_DIR+\"/\"+folderName+\"/\"+ticker+\"_\"+folderName+\".csv\"\n",
    "    table= pd.read_csv(dir,index_col=0,low_memory=False)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd2f54c8-01b7-42bb-8d1e-c34286c6ea8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes in a table and the number of lag days as input\n",
    "## Create or edit a column with lag data on a specified number of entries earlier. \n",
    "## Note, the lag is based on entries not on actual date\n",
    "def lag_feature(table, lag, column=\"Close\"):\n",
    "    shifted = table[column].shift(lag)\n",
    "    return shifted\n",
    "    \n",
    "def simple_moving_average(table,window_in,column=\"Close\"):\n",
    "    sma = table[column].rolling(window=window_in).mean()\n",
    "    return sma\n",
    "\n",
    "def rolling_median(table,window_in,column=\"Close\"):\n",
    "    median = table[column].rolling(window=window_in).median()\n",
    "    return median\n",
    "    \n",
    "def exponential_moving_average(table,window,column=\"Close\"):\n",
    "    ema = table[column].ewm(span=window, adjust=False).mean()\n",
    "    return ema\n",
    "\n",
    "def relative_strength_index(table,window=14):\n",
    "    change=table[\"Close\"].diff()\n",
    "    change.dropna(inplace=True)\n",
    "    \n",
    "    change_up = change.copy()\n",
    "    change_down = change.copy()\n",
    "    change_up[change_up<0] = 0\n",
    "    change_down[change_down>0] = 0\n",
    "\n",
    "    avg_up = change_up.rolling(window).mean()\n",
    "    avg_down = change_down.rolling(window).mean().abs()\n",
    "    rsi = 100 * avg_up / (avg_up + avg_down)\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def plot_RSI(table,window=14):\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    plt.rcParams['figure.figsize'] = (20, 20)\n",
    "    ax1 = plt.subplot2grid((10,1), (0,0), rowspan = 4, colspan = 1)\n",
    "    ax2 = plt.subplot2grid((10,1), (5,0), rowspan = 4, colspan = 1)\n",
    "    ax1.plot(table['Close'], linewidth=2)\n",
    "    company=table[\"Company\"].iloc[1]\n",
    "    ax1.set_title(f'{company} Close Price')\n",
    "    ax2.set_title('Relative Strength Index')\n",
    "    ax2.plot(table[f\"RSI_{window}\"], color='orange', linewidth=1)\n",
    "    # Add two horizontal lines, signalling the buy and sell ranges.\n",
    "    # Oversold\n",
    "    ax2.axhline(30, linestyle='--', linewidth=1.5, color='green')\n",
    "    # Overbought\n",
    "    ax2.axhline(70, linestyle='--', linewidth=1.5, color='red')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def bollinger_band(table,window=15,column=\"Close\",k=2):\n",
    "    sma=simple_moving_average(table,window)\n",
    "    rolling_std = table[column].rolling(window=window).std()\n",
    "    bollinger=pd.DataFrame({f\"Bollinger_Middle_{window}\":sma})\n",
    "    bollinger[f\"Bollinger_Upper_{window}\"] = sma + (k * rolling_std)\n",
    "    bollinger[f\"Bollinger_Lower_{window}\"] = sma - (k * rolling_std)\n",
    "    bollinger.index=table.index\n",
    "    return bollinger\n",
    "    \n",
    "def plot_bollinger_band(table,window=15):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot the stock price and bands\n",
    "    plt.plot(table.index, table['Close'], label='Stock Price', color='blue', linewidth=1.5)\n",
    "    plt.plot(table.index, table[f\"Bollinger_Upper_{window}\"], label='Upper Band', color='green', linewidth=1)\n",
    "    plt.plot(table.index, table[f\"Bollinger_Lower_{window}\"], label='Lower Band', color='green', linewidth=1)\n",
    "    plt.plot(table.index, table[f\"Bollinger_Middle_{window}\"], label='Rolling Mean', color='red', linewidth=1.5)\n",
    "    \n",
    "    # Shaded area between upper and lower band\n",
    "    plt.fill_between(table.index,\n",
    "                     table[f\"Bollinger_Lower_{window}\"],\n",
    "                     table[f\"Bollinger_Upper_{window}\"],\n",
    "                     color='gray',\n",
    "                     alpha=0.2)\n",
    "    \n",
    "    plt.title(f'Bollinger Bands (Window = {window})')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.xticks([], [])\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "def moving_average_convergence_divergence(table, short_window=12, long_window=26, signal_window=9, column='Close'):\n",
    "    short_ema = exponential_moving_average(table,short_window,column=\"Close\")\n",
    "    long_ema = exponential_moving_average(table,long_window,column=\"Close\")\n",
    "    macd_line=short_ema-long_ema\n",
    "    macd=pd.DataFrame({\"MACD_Line\":macd_line})\n",
    "    macd['MACD_Signal'] = macd['MACD_Line'].ewm(span=signal_window, adjust=False).mean()\n",
    "    macd['MACD_Histogram'] = macd['MACD_Line'] - macd['MACD_Signal']\n",
    "    macd.index=table.index\n",
    "    return macd\n",
    "\n",
    "def plot_MACD(table):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "\n",
    "    # Plot MACD and Signal\n",
    "    plt.plot(table.index, table['MACD_Line'], label='MACD', color='blue', linewidth=1.5)\n",
    "    plt.plot(table.index, table['MACD_Signal'], label='Signal Line', color='red', linewidth=1.5)\n",
    "    \n",
    "    # Bar plot for histogram\n",
    "    plt.bar(table.index, table['MACD_Histogram'], label='Histogram', color='gray', alpha=0.4,width=1,)\n",
    "    \n",
    "    plt.title('MACD Indicator')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('MACD')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks([], [])\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def volatility(table, window=20, column=\"Close\"):\n",
    "    returns = table[column].pct_change()\n",
    "    returns = returns.rolling(window=window).std()\n",
    "    return returns\n",
    "\n",
    "aapl=read_CSV_File(\"AAPL\",\"metadata\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09699690-efed-4996-8e73-53b139f29114",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in TICKERS:\n",
    "    data=read_CSV_File(ticker,\"metadata\")\n",
    "    data[\"Lag_1\"]=lag_feature(data,1)\n",
    "    data[\"Lag_3\"]=lag_feature(data,3)\n",
    "    data[\"Lag_5\"]=lag_feature(data,5)\n",
    "    data[\"SMA_15\"]=simple_moving_average(data,15)\n",
    "    data[\"SMA_30\"]=simple_moving_average(data,30)\n",
    "    data[\"Rolling_Median_15\"]=rolling_median(data,15)\n",
    "    data[\"EMA_15\"]=exponential_moving_average(data,15)\n",
    "    data[\"EMA_30\"]=exponential_moving_average(data,30)\n",
    "    data[\"RSI_15\"]=relative_strength_index(data,15)\n",
    "    data[\"RSI_30\"]=relative_strength_index(data,30)\n",
    "    bollinger=bollinger_band(data,window=15)\n",
    "    data[\"Bollinger_Middle_15\"]=bollinger[\"Bollinger_Middle_15\"]\n",
    "    data[\"Bollinger_Upper_15\"]=bollinger[\"Bollinger_Upper_15\"]\n",
    "    data[\"Bollinger_Lower_15\"]=bollinger[\"Bollinger_Lower_15\"]\n",
    "    macd=moving_average_convergence_divergence(data)\n",
    "    data[\"MACD_Line\"]=macd[\"MACD_Line\"]\n",
    "    data[\"MACD_Signal\"]=macd[\"MACD_Signal\"]\n",
    "    data[\"MACD_Histogram\"]=macd[\"MACD_Histogram\"]\n",
    "    data[\"Volatility_20\"]=volatility(data,20)\n",
    "    data.to_csv(os.path.join(BASE_DIR, \"feature\", f\"{ticker}_feature.csv\"), index=True)\n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8eb45630-7fcb-446c-8442-78747eee37a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrices = []\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    data = read_CSV_File(ticker,\"correlation\")\n",
    "    correlation_matrices.append(data)\n",
    "\n",
    "# Ensure all matrices have the same index/column order\n",
    "base_index = correlation_matrices[0].index\n",
    "correlation_matrices = [df.loc[base_index, base_index] for df in correlation_matrices]\n",
    "\n",
    "    # Stack into 3D numpy array\n",
    "stacked = np.stack([df.values for df in correlation_matrices])\n",
    "\n",
    "    # Compute mean along axis 0\n",
    "avg_matrix = np.mean(stacked, axis=0)\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "avg_corr = pd.DataFrame(avg_matrix, index=base_index, columns=base_index)\n",
    "avg_corr=avg_corr.sort_values(by='Return_5d', ascending=False)\n",
    "avg_corr.to_csv(os.path.join(BASE_DIR, \"correlation\", \"MASTER_correlation.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7aacfc1-4dc1-44f6-ab10-f0f5e854b951",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in TICKERS:\n",
    "    data=read_CSV_File(ticker,\"feature\")\n",
    "    data['Return_1d'] = data['Close'].pct_change().shift(-1)\n",
    "    data['Return_5d'] = (data['Close'].shift(-5) - data['Close']) / data['Close']\n",
    "    selected = [ 'Volume','DilutedEPS', 'PE', 'Revenue', 'CashFlow', 'EBITDA', 'GrossProfit',\n",
    "       'OperatingMargin', 'ROE', 'DebtToEquity','Revenue_perShare', 'CashFlow_perShare',\n",
    "       'EBITDA_perShare', 'GrossProfit_perShare', 'Lag_1', 'Lag_3', 'Lag_5',\n",
    "       'SMA_15', 'SMA_30', 'Rolling_Median_15', 'EMA_15', 'EMA_30', 'RSI_15',\n",
    "       'RSI_30', 'Bollinger_Middle_15', 'Bollinger_Upper_15',\n",
    "       'Bollinger_Lower_15', 'MACD_Line', 'MACD_Signal', 'MACD_Histogram',\n",
    "       'Volatility_20',\"Close\"]\n",
    "    correlation_matrix = data[selected+[\"Return_5d\",\"Return_1d\"]].corr()\n",
    "    mask = np.tril(np.ones(correlation_matrix.shape)).astype(bool)\n",
    "    correlation_matrix = correlation_matrix.mask(mask)\n",
    "    correlation_matrix.to_csv(os.path.join(BASE_DIR, \"correlation\", f\"{ticker}_correlation.csv\"), index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3b7e1ef2-9485-432b-ae8a-f6a609a738b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in TICKERS:\n",
    "    data = read_CSV_File(ticker, \"feature\")\n",
    "    \n",
    "    # Calculate returns\n",
    "    data['Return_1d'] = data['Close'].pct_change().shift(-1)\n",
    "    data['Return_5d'] = (data['Close'].shift(-5) - data['Close']) / data['Close']\n",
    "\n",
    "    # Drop irrelevant or redundant columns\n",
    "    data = data.drop(columns=[\n",
    "        'Company', 'High', 'Low', 'Open', 'Lag_1', 'Lag_3', 'Lag_5', 'SMA_15', 'SMA_30',\n",
    "        'Rolling_Median_15', 'EMA_15', 'EMA_30', 'RSI_15', 'RSI_30', 'Bollinger_Middle_15',\n",
    "        'Bollinger_Upper_15', 'Bollinger_Lower_15', 'MACD_Line', 'MACD_Signal',\n",
    "        'MACD_Histogram', 'Volatility_20', 'Year'\n",
    "    ], errors='ignore')\n",
    "\n",
    "    # Drop rows with NaNs created from returns\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Targets\n",
    "    y_1d = data['Return_1d']\n",
    "    y_5d = data['Return_5d']\n",
    "\n",
    "    # Feature sets\n",
    "    X_1 = data.drop(columns=['Return_1d', 'Return_5d'])\n",
    "    X_5 = data.drop(columns=['Return_1d', 'Return_5d'])\n",
    "\n",
    "    # Train models\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_1, y_1d)\n",
    "    importances_1 = model.feature_importances_\n",
    "\n",
    "    model.fit(X_5, y_5d)\n",
    "    importances_5 = model.feature_importances_\n",
    "\n",
    "    # Build output DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Indicator': X_1.columns,\n",
    "        'Importance_Return1d': importances_1,\n",
    "        'Importance_Return5d': importances_5\n",
    "    }).sort_values(by='Importance_Return5d', ascending=False)\n",
    "\n",
    "    # Save\n",
    "    feature_importance.to_csv(\n",
    "        os.path.join(BASE_DIR, \"indicator_importance\", f\"{ticker}_indicator_importance.csv\"),\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "58fcb978-66cb-40d9-b96e-d99d9400bb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the average importance of each indicator across 10 companies\n",
    "combined_1d = None\n",
    "combined_5d = None\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Load each ticker's importance file\n",
    "    df = read_CSV_File(ticker, \"indicator_importance\")\n",
    "    df = df.reset_index()\n",
    "    # Extract and rename both columns\n",
    "    imp_1d = df[['Indicator', 'Importance_Return1d']].rename(columns={'Importance_Return1d': f'Importance_1d_{ticker}'})\n",
    "    imp_5d = df[['Indicator', 'Importance_Return5d']].rename(columns={'Importance_Return5d': f'Importance_5d_{ticker}'})\n",
    "    \n",
    "    # Merge across tickers\n",
    "    if combined_1d is None:\n",
    "        combined_1d = imp_1d\n",
    "        combined_5d = imp_5d\n",
    "    else:\n",
    "        combined_1d = combined_1d.merge(imp_1d, on='Indicator', how='outer')\n",
    "        combined_5d = combined_5d.merge(imp_5d, on='Indicator', how='outer')\n",
    "\n",
    "# Compute averages\n",
    "combined_1d['Avg_Importance_1d'] = combined_1d.drop(columns=['Indicator']).mean(axis=1)\n",
    "combined_5d['Avg_Importance_5d'] = combined_5d.drop(columns=['Indicator']).mean(axis=1)\n",
    "\n",
    "# Merge both tables\n",
    "master_df = combined_1d.merge(combined_5d, on='Indicator', how='outer')\n",
    "\n",
    "# Optional: Reorder columns\n",
    "ordered_cols = (\n",
    "    ['Indicator', 'Avg_Importance_1d', 'Avg_Importance_5d'] +\n",
    "    [col for col in master_df.columns if col not in ['Indicator', 'Avg_Importance_1d', 'Avg_Importance_5d']]\n",
    ")\n",
    "master_df = master_df[ordered_cols]\n",
    "\n",
    "# Sort by average importance (e.g. 5-day)\n",
    "master_df = master_df.sort_values('Avg_Importance_5d', ascending=False)\n",
    "\n",
    "# Save\n",
    "master_df.to_csv(os.path.join(BASE_DIR, \"indicator_importance\", \"MASTER_indicator_importance.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "635919ca-179e-41a7-b05a-1624279b8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ticker in TICKERS:\n",
    "    data = read_CSV_File(ticker, \"feature\")\n",
    "    \n",
    "    # Calculate returns\n",
    "    data['Return_1d'] = data['Close'].pct_change().shift(-1)\n",
    "    data['Return_5d'] = (data['Close'].shift(-5) - data['Close']) / data['Close']\n",
    "\n",
    "    # Drop irrelevant or redundant columns\n",
    "    data=data.drop(columns=['Company','Volume','High', 'Low', 'Open', 'Dividends', 'Stock Splits', 'Year', 'DilutedEPS',\n",
    "           'PE', 'Revenue', 'CashFlow', 'EBITDA', 'GrossProfit', 'OperatingMargin',\n",
    "           'ROE', 'DebtToEquity', 'HasDividend', 'Revenue_perShare',\n",
    "           'CashFlow_perShare', 'EBITDA_perShare', 'GrossProfit_perShare','Lag_1',\"Year\"])\n",
    "\n",
    "    # Drop rows with NaNs created from returns\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Targets\n",
    "    y_1d = data['Return_1d']\n",
    "    y_5d = data['Return_5d']\n",
    "\n",
    "    # Feature sets\n",
    "    X_1 = data.drop(columns=['Return_1d', 'Return_5d'])\n",
    "    X_5 = data.drop(columns=['Return_1d', 'Return_5d'])\n",
    "\n",
    "    # Train models\n",
    "    model = RandomForestRegressor()\n",
    "    model.fit(X_1, y_1d)\n",
    "    importances_1 = model.feature_importances_\n",
    "\n",
    "    model.fit(X_5, y_5d)\n",
    "    importances_5 = model.feature_importances_\n",
    "\n",
    "    # Build output DataFrame\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_1.columns,\n",
    "        'Importance_Return1d': importances_1,\n",
    "        'Importance_Return5d': importances_5\n",
    "    }).sort_values(by='Importance_Return5d', ascending=False)\n",
    "\n",
    "    # Save\n",
    "    feature_importance.to_csv(\n",
    "        os.path.join(BASE_DIR, \"feature_importance\", f\"{ticker}_feature_importance.csv\"),\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93007ffe-d6ac-4556-bac7-4111fb632f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.DataFrame()\n",
    "for ticker in TICKERS:\n",
    "    data=read_CSV_File(ticker,\"feature_importance\")\n",
    "    if combined.empty:\n",
    "        combined = data.copy()\n",
    "    else:\n",
    "        combined = combined.merge(data, on=\"Feature\", how=\"outer\", suffixes=(\"\", f\"_{ticker}\"))\n",
    "\n",
    "combined['Avg_Importance'] = combined.select_dtypes(include='number').mean(axis=1)\n",
    "combined.sort_values(\"Avg_Importance\", ascending=False, inplace=True)\n",
    "\n",
    "combined = combined.rename(columns={\"Importance\": \"Importance_AAPL\"})\n",
    "last_col = combined.columns[-1]\n",
    "combined = combined[[last_col] + list(combined.columns[:-1])]\n",
    "combined.to_csv(os.path.join(BASE_DIR, \"feature_importance\", \"MASTER_feature_importance.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "72af6de7-82e6-4e10-8e5b-16b79aa411ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute the average importance of each indicator across 10 companies\n",
    "combined_1d = None\n",
    "combined_5d = None\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    # Load each ticker's importance file\n",
    "    df = read_CSV_File(ticker, \"feature_importance\")\n",
    "    df = df.reset_index()\n",
    "    # Extract and rename both columns\n",
    "    imp_1d = df[['Feature', 'Importance_Return1d']].rename(columns={'Importance_Return1d': f'Importance_1d_{ticker}'})\n",
    "    imp_5d = df[['Feature', 'Importance_Return5d']].rename(columns={'Importance_Return5d': f'Importance_5d_{ticker}'})\n",
    "    \n",
    "    # Merge across tickers\n",
    "    if combined_1d is None:\n",
    "        combined_1d = imp_1d\n",
    "        combined_5d = imp_5d\n",
    "    else:\n",
    "        combined_1d = combined_1d.merge(imp_1d, on='Feature', how='outer')\n",
    "        combined_5d = combined_5d.merge(imp_5d, on='Feature', how='outer')\n",
    "\n",
    "# Compute averages\n",
    "combined_1d['Avg_Importance_1d'] = combined_1d.drop(columns=['Feature']).mean(axis=1)\n",
    "combined_5d['Avg_Importance_5d'] = combined_5d.drop(columns=['Feature']).mean(axis=1)\n",
    "\n",
    "# Merge both tables\n",
    "master_df = combined_1d.merge(combined_5d, on='Feature', how='outer')\n",
    "\n",
    "# Optional: Reorder columns\n",
    "ordered_cols = (\n",
    "    ['Feature', 'Avg_Importance_1d', 'Avg_Importance_5d'] +\n",
    "    [col for col in master_df.columns if col not in ['Indicator', 'Avg_Importance_1d', 'Avg_Importance_5d']]\n",
    ")\n",
    "master_df = master_df[ordered_cols]\n",
    "\n",
    "# Sort by average importance (e.g. 5-day)\n",
    "master_df = master_df.sort_values('Avg_Importance_5d', ascending=False)\n",
    "\n",
    "# Save\n",
    "master_df.to_csv(os.path.join(BASE_DIR, \"feature_importance\", \"MASTER_feature_importance.csv\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd53c45-0f2f-4881-8446-0bdfe8ec626b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI_Trader)",
   "language": "python",
   "name": "ai_trader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
