{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "46e6a8ec-b3ca-4ee3-95d7-80e5485bdd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime\n",
    "import warnings\n",
    "BASE_DIR=\"/Users/dannyyu/Desktop/AI_Trader/data\"\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'NVDA', 'TSLA', 'INTC', 'AMD', 'IBM']\n",
    "\n",
    "\n",
    "def read_CSV_File(ticker,folderName):\n",
    "    dir = BASE_DIR+\"/\"+folderName+\"/\"+ticker+\"_\"+folderName+\".csv\"\n",
    "    table= pd.read_csv(dir,index_col=0)\n",
    "    return table\n",
    "\n",
    "def has_divident(metadata):\n",
    "    metadata[\"Year\"] = pd.to_datetime(metadata.index, utc=True).year #pd.to_datetime(metadata['Date'], utc=True).dt.year\n",
    "    dividend_by_year = metadata.groupby('Year')['Dividends'].apply(lambda x: int((x > 0).any()))\n",
    "    metadata['HasDividend'] = metadata['Year'].map(dividend_by_year)\n",
    "    return metadata\n",
    "\n",
    "def missing_values(table):\n",
    "    total_rows = len(table)\n",
    "    hasMissing=False\n",
    "    print(\"Analyzing \"+table.iloc[1][\"Company\"])\n",
    "    for column in table.columns:\n",
    "        num_missing = table[column].isna().sum()\n",
    "        if num_missing == 0:\n",
    "            #print(f\"ðŸ“Š '{column}': no missing values\")\n",
    "            continue  # skip columns with no missing data\n",
    "        else:\n",
    "            hasMissing=True\n",
    "        percent_missing = 100 * num_missing / total_rows\n",
    "        print(f\"ðŸ“Š '{column}': {num_missing} missing ({percent_missing:.2f}%)\")\n",
    "    if not hasMissing:\n",
    "        print(table.iloc[1][\"Company\"] + \" has no missing values\")\n",
    "    return hasMissing\n",
    "\n",
    "def get_closest_price(ticker_obj, target_date, lookback_days=3):\n",
    "    try:\n",
    "        start_date = target_date - pd.Timedelta(days=lookback_days)\n",
    "        end_date = target_date + pd.Timedelta(days=1)\n",
    "\n",
    "        hist = ticker_obj.history(start=start_date, end=end_date)\n",
    "        \n",
    "        if not hist.empty:\n",
    "            last_row = hist.iloc[-1]\n",
    "            timestamp_str = hist.index[-1].strftime(\"%Y-%m-%d\")\n",
    "           # print(\"Data retrieved on date \"+timestamp_str)\n",
    "            return last_row['Close']\n",
    "        else:\n",
    "            print(f\"No price data found for {ticker_obj.ticker} between {start_date.date()} and {end_date.date()}\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving price for {ticker_obj.ticker} near {target_date.date()}: {e}\")\n",
    "        return None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "92f825b6-d0ff-479d-9580-f3f7d6d3fd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "###estimating the missing values for Diluted EPS and the PE ratio for IBM\n",
    "#estimate the EPS for 2024 using the currently shares outstanding as a proxy\n",
    "ibm=read_CSV_File(\"IBM\",\"metadata\")\n",
    "ibm_ticker= yf.Ticker(\"IBM\")\n",
    "ibm_fin=ibm_ticker.financials\n",
    "net_income = ibm_fin[\"2024-12-31\"][\"Net Income\"]\n",
    "shares_outstanding = ibm_ticker.info.get(\"sharesOutstanding\", None)\n",
    "eps_estimate=net_income / shares_outstanding\n",
    "# Estimating the PE ratio for 2024 using the EPS estimation from the previous step\n",
    "hist_price=get_closest_price(ibm_ticker, pd.to_datetime(\"2024-12-31\"))\n",
    "pe_ratio = hist_price / eps_estimate\n",
    "\n",
    "ibm['DilutedEPS'] =ibm['DilutedEPS'].fillna(eps_estimate)\n",
    "ibm[\"DilutedEPS\"] = ibm[\"DilutedEPS\"].round(2)\n",
    "ibm['PE'] = ibm['PE'].fillna(pe_ratio)\n",
    "ibm['PE'] = ibm['PE'].round(5)\n",
    "ibm.to_csv(os.path.join(BASE_DIR, \"metadata\",\"IBM_metadata.csv\"), index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "97b5478d-7685-471d-bec7-777b4db0a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "##estimate the missing PE ratio for INTC using today's price\n",
    "intc=read_CSV_File(\"INTC\",\"metadata\")\n",
    "intc_ticker= yf.Ticker(\"INTC\")\n",
    "price=intc_ticker.history(period=\"1d\")[\"Close\"].iloc[-1]\n",
    "eps=intc.iloc[-1][\"DilutedEPS\"]\n",
    "pe=price/eps\n",
    "intc[\"PE\"]=intc[\"PE\"].fillna(pe)\n",
    "intc['PE'] = intc['PE'].round(5)\n",
    "intc.to_csv(os.path.join(BASE_DIR, \"metadata\",\"INTC_metadata.csv\"), index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "dfa03049-e93d-4277-a287-a28b044acbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing AAPL\n",
      "AAPL has no missing values\n",
      "Analyzing MSFT\n",
      "MSFT has no missing values\n",
      "Analyzing GOOGL\n",
      "GOOGL has no missing values\n",
      "Analyzing AMZN\n",
      "AMZN has no missing values\n",
      "Analyzing META\n",
      "META has no missing values\n",
      "Analyzing NVDA\n",
      "NVDA has no missing values\n",
      "Analyzing TSLA\n",
      "TSLA has no missing values\n",
      "Analyzing INTC\n",
      "INTC has no missing values\n",
      "Analyzing AMD\n",
      "AMD has no missing values\n",
      "Analyzing IBM\n",
      "IBM has no missing values\n"
     ]
    }
   ],
   "source": [
    "for ticker in TICKERS:\n",
    "    table=read_CSV_File(ticker,\"metadata\")\n",
    "    table=has_divident(table)\n",
    "    table[\"DebtToEquity\"]=table[\"DebtToEquity\"].round(3)\n",
    "    table[\"PE\"]=table[\"PE\"].round(6)\n",
    "    table[\"ROE\"]=table[\"ROE\"].round(3)\n",
    "    missing_value=missing_values(table)\n",
    "    table.to_csv(os.path.join(BASE_DIR, \"metadata\", f\"{ticker}_metadata.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "aae7a46a-65a8-40fd-9539-1fe4651770ba",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combine_metadata_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[489], line 64\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror occured\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m master_df \u001b[38;5;241m=\u001b[39m \u001b[43mcombine_metadata_files\u001b[49m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     65\u001b[0m master_df\u001b[38;5;241m.\u001b[39mhead\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combine_metadata_files' is not defined"
     ]
    }
   ],
   "source": [
    "def get_shares_outstanding(table, year):\n",
    "    ticker_symbol = table['Company'].iloc[0]\n",
    "    ticker = yf.Ticker(ticker_symbol)\n",
    "\n",
    "    current_year = datetime.datetime.now().year\n",
    "    if year == current_year:\n",
    "        shares = ticker.info.get('sharesOutstanding', None)\n",
    "        return round(shares, 0)\n",
    "    else:\n",
    "        income = ticker.financials\n",
    "        income.columns = pd.to_datetime(income.columns)\n",
    "        matching_cols = [col for col in income.columns if col.year == year]\n",
    "        col = matching_cols[0]\n",
    "        net_income = income.at['Net Income', col]\n",
    "        row = table[table[\"Year\"] == year]\n",
    "        eps = row['DilutedEPS'].iloc[0]\n",
    "        estimated_shares = net_income / eps\n",
    "        return round(estimated_shares,0)\n",
    "\n",
    "\n",
    "#Adds per-share normalized versions of Revenue, CashFlow, EBITDA, and GrossProfit using estimated shares outstanding for each row.\n",
    "def normalize_per_share(table):\n",
    "    table['Revenue_perShare'] = None\n",
    "    table['CashFlow_perShare'] = None\n",
    "    table['EBITDA_perShare'] = None\n",
    "    table['GrossProfit_perShare'] = None\n",
    "    \n",
    "    for year in table[\"Year\"].unique():\n",
    "        mask = (table['Year'] == year)\n",
    "        row_subset = table[mask]\n",
    "        shares = get_shares_outstanding(table, year)\n",
    "\n",
    "        table.loc[mask, 'Revenue_perShare']     = table.loc[mask, 'Revenue']     / shares\n",
    "        table.loc[mask, 'CashFlow_perShare']    = table.loc[mask, 'CashFlow']    / shares\n",
    "        table.loc[mask, 'EBITDA_perShare']      = table.loc[mask, 'EBITDA']      / shares\n",
    "        table.loc[mask, 'GrossProfit_perShare'] = table.loc[mask, 'GrossProfit'] / shares\n",
    "\n",
    "    per_share_cols = ['Revenue_perShare', 'CashFlow_perShare', 'EBITDA_perShare', 'GrossProfit_perShare']\n",
    "    table[per_share_cols] = table[per_share_cols].astype(float).round(5)\n",
    "    return table\n",
    "\n",
    "\n",
    "for ticker in TICKERS:\n",
    "    table=read_CSV_File(ticker,\"metadata\")\n",
    "    table=normalize_per_share(table)\n",
    "    table.to_csv(os.path.join(BASE_DIR, \"metadata\", f\"{ticker}_metadata.csv\"), index=True)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "f87e6907-eae3-4843-8de7-b377d9f4c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combines all the metadata files in the given direcotr\n",
    "def combine_metadata_files(folderName=\"metadata\"):\n",
    "    combined=[]\n",
    "    for file in os.listdir(os.path.join(BASE_DIR, folderName)):\n",
    "        if file.endswith(\"_metadata.csv\"):\n",
    "            ticker = file.split(\"_\")[0]\n",
    "            df = read_CSV_File(ticker, folderName)\n",
    "            combined.append(df)\n",
    "\n",
    "    if combined:\n",
    "        master_df = pd.concat(combined, ignore_index=False)\n",
    "        return master_df\n",
    "    else:\n",
    "        print(\"error occured\")\n",
    "        return None\n",
    "\n",
    "master_df = combine_metadata_files()\n",
    "master_df.to_csv(os.path.join(BASE_DIR, \"metadata\", \"MASTER_metadata.csv\"), index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "28ce7d94-f56e-4e8f-b11b-e86f4d0bbd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_percentile(table):\n",
    "    indicators = [\"OperatingMargin\",\"ROE\",\"DebtToEquity\",\"Revenue_perShare\",\"CashFlow_perShare\",\"EBITDA_perShare\",\"GrossProfit_perShare\"]\n",
    "    for indicator in indicators:\n",
    "        table[f\"{indicator}_Pct\"]=table.groupby('Year')[indicator].rank(pct=True)\n",
    "        table[f\"{indicator}_Pct\"]=table[f\"{indicator}_Pct\"].round(5)\n",
    "    return table\n",
    "\n",
    "master=read_CSV_File(\"MASTER\", \"metadata\")\n",
    "master=compute_percentile(master)\n",
    "master.to_csv(os.path.join(BASE_DIR, \"metadata\", \"MASTER_metadata.csv\"), index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e2536-6caf-412a-8ea1-63230ba15cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AI_Trader)",
   "language": "python",
   "name": "ai_trader"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
